===== START Thu 01/15/2026 10:23:21.26 ===== 
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
[codecarbon INFO @ 10:23:54] [setup] RAM Tracking...
[codecarbon INFO @ 10:23:54] [setup] CPU Tracking...
[codecarbon WARNING @ 10:23:54] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 10:23:55] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 10:23:55] [setup] GPU Tracking...
[codecarbon INFO @ 10:23:55] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 10:23:56] >>> Tracker's metadata:
[codecarbon INFO @ 10:23:56]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 10:23:56]   Python version: 3.12.7
[codecarbon INFO @ 10:23:56]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 10:23:56]   Available RAM : 126.630 GB
[codecarbon INFO @ 10:23:56]   CPU count: 56
[codecarbon INFO @ 10:23:56]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 10:23:56]   GPU count: 2
[codecarbon INFO @ 10:23:56]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 10:23:59] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 10:24:00] Energy consumed for RAM : 0.000019 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 10:24:00] Energy consumed for all CPUs : 0.000041 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 10:24:00] Energy consumed for all GPUs : 0.000007 kWh. Total GPU Power : 16.45529022630004 W
[codecarbon INFO @ 10:24:00] 0.000067 kWh of electricity used since the beginning.
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
[codecarbon INFO @ 10:50:42] [setup] RAM Tracking...
[codecarbon INFO @ 10:50:42] [setup] CPU Tracking...
[codecarbon WARNING @ 10:50:42] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 10:50:44] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 10:50:44] [setup] GPU Tracking...
[codecarbon INFO @ 10:50:44] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 10:50:44] >>> Tracker's metadata:
[codecarbon INFO @ 10:50:44]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 10:50:44]   Python version: 3.12.7
[codecarbon INFO @ 10:50:44]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 10:50:44]   Available RAM : 126.630 GB
[codecarbon INFO @ 10:50:44]   CPU count: 56
[codecarbon INFO @ 10:50:44]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 10:50:44]   GPU count: 2
[codecarbon INFO @ 10:50:44]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 10:50:47] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 10:50:48] Energy consumed for RAM : 0.000017 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 10:50:48] Energy consumed for all CPUs : 0.000037 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 10:50:48] Energy consumed for all GPUs : 0.000006 kWh. Total GPU Power : 17.031359352585884 W
[codecarbon INFO @ 10:50:48] 0.000060 kWh of electricity used since the beginning.
[codecarbon INFO @ 10:51:42] [setup] RAM Tracking...
[codecarbon INFO @ 10:51:42] [setup] CPU Tracking...
[codecarbon WARNING @ 10:51:42] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 10:51:44] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 10:51:44] [setup] GPU Tracking...
[codecarbon INFO @ 10:51:44] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 10:51:44] >>> Tracker's metadata:
[codecarbon INFO @ 10:51:44]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 10:51:44]   Python version: 3.12.7
[codecarbon INFO @ 10:51:44]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 10:51:44]   Available RAM : 126.630 GB
[codecarbon INFO @ 10:51:44]   CPU count: 56
[codecarbon INFO @ 10:51:44]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 10:51:44]   GPU count: 2
[codecarbon INFO @ 10:51:44]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 10:51:47] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 10:51:48] Energy consumed for RAM : 0.000011 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 10:51:48] Energy consumed for all CPUs : 0.000023 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 10:51:48] Energy consumed for all GPUs : 0.000004 kWh. Total GPU Power : 16.7151435712092 W
[codecarbon INFO @ 10:51:48] 0.000038 kWh of electricity used since the beginning.
################################################################
#                              batch_size: 64                  #
#                         test_batch_size: 64                  #
#                                  epochs: 10                  #
#                               optimizer: SGD                 #
#                                      lr: 0.001               #
#                                momentum: 0.5                 #
#                                    seed: 1                   #
#                             num_clients: 10                  #
#                                   scale: 2                   #
#                                 dataset: plant               #
#                             loader_type: dirichlet           #
#                                      AR: mudhog              #
#                                    side: both                #
#                                     PDR: 1.0                 #
#                                  attack: backdoor pls->14    #
#                          label_flipping: uni                 #
#                         experiment_name: split_fed_backdoor_pls_to_14_inner_epochs=5_epochs=10_PDR=1.0_scale=2_mudhog#
#                            inner_epochs: 5                   #
#                                   setup: split_fed           #
#                                   alpha: 0.5                 #
################################################################
NVIDIA RTX A5000
---------split_fed_backdoor_pls_to_14_inner_epochs=5_epochs=10_PDR=1.0_scale=2_mudhog----------
initialize a data loader
Using cuda
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 96.794 	Loss: 0.0954[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.194 	Loss: 83176492866.9538[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 89.844 	Loss: 0.2995[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client1 Test => 	Acc: 2.194 	Loss: 58695842201.6000[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 21.701 	Loss: 4.9984[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 29.340 	Loss: 2.3674[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 42.969 	Loss: 2.0736[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 43.056 	Loss: 1.9935[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 42.101 	Loss: 1.9564[00m
[92m  Client2 Test => 	Acc: 25.355 	Loss: 3.2112[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 27.300 	Loss: 8.1722[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 31.120 	Loss: 2.2429[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 37.066 	Loss: 1.9482[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 60.113 	Loss: 1.3946[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 70.790 	Loss: 1.0508[00m
[92m  Client3 Test => 	Acc: 27.657 	Loss: 2.8221[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 14.941 	Loss: 5.5243[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 19.727 	Loss: 2.7049[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 38.965 	Loss: 1.9154[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 50.000 	Loss: 1.5831[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 57.031 	Loss: 1.2884[00m
[92m  Client4 Test => 	Acc: 31.539 	Loss: 3.0490[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 19.661 	Loss: 2.9271[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 30.339 	Loss: 2.3486[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 30.859 	Loss: 2.0911[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 43.490 	Loss: 1.8266[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 38.932 	Loss: 1.8961[00m
[92m  Client5 Test => 	Acc: 22.918 	Loss: 3.1405[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 27.750 	Loss: 2.8693[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 38.094 	Loss: 1.9483[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 55.125 	Loss: 1.4642[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 65.125 	Loss: 1.1378[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 71.250 	Loss: 0.9270[00m
[92m  Client6 Test => 	Acc: 51.158 	Loss: 1.8570[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 19.922 	Loss: 4.7420[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 23.828 	Loss: 2.5055[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 27.578 	Loss: 2.1426[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 32.656 	Loss: 1.9642[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 40.234 	Loss: 1.8670[00m
[92m  Client7 Test => 	Acc: 22.542 	Loss: 3.4771[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 17.057 	Loss: 5.6773[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 23.568 	Loss: 2.1984[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 27.604 	Loss: 2.1212[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 23.828 	Loss: 2.1742[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 33.594 	Loss: 1.9709[00m
[92m  Client8 Test => 	Acc: 9.900 	Loss: 3.9412[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 19.651 	Loss: 3.1770[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 28.726 	Loss: 2.0580[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 46.154 	Loss: 1.6629[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 55.228 	Loss: 1.4409[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 62.560 	Loss: 1.2138[00m
[92m  Client9 Test => 	Acc: 49.537 	Loss: 2.1457[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[0. 0. 1. 1. 1. 1. 1. 0. 1. 1.]
Client-side aggregation done
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Server-side aggregation done
[92m  Client0 Test => 	Acc: 2.194 	Loss: 2.7525[00m
 Train: Round   0, Avg Accuracy 61.649 | Avg Loss 1.217
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.1191[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.201 	Loss: 1124978.3942[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.4800[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client1 Test => 	Acc: 2.194 	Loss: 9583.2899[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 38.281 	Loss: 2.2973[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 42.361 	Loss: 2.0361[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 45.747 	Loss: 1.7374[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 56.163 	Loss: 1.4740[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 62.934 	Loss: 1.2093[00m
[92m  Client2 Test => 	Acc: 38.157 	Loss: 3.9794[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 32.943 	Loss: 2.0069[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 64.106 	Loss: 1.2423[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 73.394 	Loss: 0.9653[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 75.781 	Loss: 0.8878[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 76.997 	Loss: 0.7957[00mC:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
[codecarbon INFO @ 11:18:41] [setup] RAM Tracking...
[codecarbon INFO @ 11:18:41] [setup] CPU Tracking...
[codecarbon WARNING @ 11:18:41] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 11:18:43] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 11:18:43] [setup] GPU Tracking...
[codecarbon INFO @ 11:18:43] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 11:18:43] >>> Tracker's metadata:
[codecarbon INFO @ 11:18:43]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 11:18:43]   Python version: 3.12.7
[codecarbon INFO @ 11:18:43]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 11:18:43]   Available RAM : 126.630 GB
[codecarbon INFO @ 11:18:43]   CPU count: 56
[codecarbon INFO @ 11:18:43]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 11:18:43]   GPU count: 2
[codecarbon INFO @ 11:18:43]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 11:18:46] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 11:18:47] Energy consumed for RAM : 0.000010 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 11:18:47] Energy consumed for all CPUs : 0.000022 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 11:18:47] Energy consumed for all GPUs : 0.000003 kWh. Total GPU Power : 16.210156747035608 W
[codecarbon INFO @ 11:18:47] 0.000036 kWh of electricity used since the beginning.
[codecarbon INFO @ 11:19:42] [setup] RAM Tracking...
[codecarbon INFO @ 11:19:42] [setup] CPU Tracking...
[codecarbon WARNING @ 11:19:42] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 11:19:44] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 11:19:44] [setup] GPU Tracking...
[codecarbon INFO @ 11:19:44] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 11:19:44] >>> Tracker's metadata:
[codecarbon INFO @ 11:19:44]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 11:19:44]   Python version: 3.12.7
[codecarbon INFO @ 11:19:44]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 11:19:44]   Available RAM : 126.630 GB
[codecarbon INFO @ 11:19:44]   CPU count: 56
[codecarbon INFO @ 11:19:44]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 11:19:44]   GPU count: 2
[codecarbon INFO @ 11:19:44]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 11:19:47] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 11:19:48] Energy consumed for RAM : 0.000009 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 11:19:48] Energy consumed for all CPUs : 0.000019 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 11:19:48] Energy consumed for all GPUs : 0.000003 kWh. Total GPU Power : 17.166764640819093 W
[codecarbon INFO @ 11:19:48] 0.000032 kWh of electricity used since the beginning.
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
[codecarbon INFO @ 11:46:37] [setup] RAM Tracking...
[codecarbon INFO @ 11:46:37] [setup] CPU Tracking...
[codecarbon WARNING @ 11:46:37] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 11:46:38] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 11:46:38] [setup] GPU Tracking...
[codecarbon INFO @ 11:46:38] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 11:46:38] >>> Tracker's metadata:
[codecarbon INFO @ 11:46:38]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 11:46:38]   Python version: 3.12.7
[codecarbon INFO @ 11:46:38]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 11:46:38]   Available RAM : 126.630 GB
[codecarbon INFO @ 11:46:38]   CPU count: 56
[codecarbon INFO @ 11:46:38]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 11:46:38]   GPU count: 2
[codecarbon INFO @ 11:46:38]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 11:46:42] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 11:46:43] Energy consumed for RAM : 0.000010 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 11:46:43] Energy consumed for all CPUs : 0.000021 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 11:46:43] Energy consumed for all GPUs : 0.000004 kWh. Total GPU Power : 21.279688232567395 W
[codecarbon INFO @ 11:46:43] 0.000035 kWh of electricity used since the beginning.
[codecarbon INFO @ 11:47:39] [setup] RAM Tracking...
[codecarbon INFO @ 11:47:39] [setup] CPU Tracking...
[codecarbon WARNING @ 11:47:39] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 11:47:41] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 11:47:41] [setup] GPU Tracking...
[codecarbon INFO @ 11:47:41] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 11:47:41] >>> Tracker's metadata:
[codecarbon INFO @ 11:47:41]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 11:47:41]   Python version: 3.12.7
[codecarbon INFO @ 11:47:41]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 11:47:41]   Available RAM : 126.630 GB
[codecarbon INFO @ 11:47:41]   CPU count: 56
[codecarbon INFO @ 11:47:41]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 11:47:41]   GPU count: 2
[codecarbon INFO @ 11:47:41]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 11:47:44] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 11:47:45] Energy consumed for RAM : 0.000008 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 11:47:45] Energy consumed for all CPUs : 0.000018 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 11:47:45] Energy consumed for all GPUs : 0.000003 kWh. Total GPU Power : 18.32149319893667 W
[codecarbon INFO @ 11:47:45] 0.000029 kWh of electricity used since the beginning.

[92m  Client3 Test => 	Acc: 30.857 	Loss: 2.6770[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 18.848 	Loss: 2.4561[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 37.695 	Loss: 1.8418[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 43.750 	Loss: 1.8132[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 51.758 	Loss: 1.4788[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 62.012 	Loss: 1.1270[00m
[92m  Client4 Test => 	Acc: 29.133 	Loss: 3.2430[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 17.448 	Loss: 2.4495[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 31.641 	Loss: 2.0333[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 44.010 	Loss: 1.7535[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 52.474 	Loss: 1.5285[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 54.688 	Loss: 1.3991[00m
[92m  Client5 Test => 	Acc: 28.353 	Loss: 2.8542[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 29.906 	Loss: 2.1158[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 28.500 	Loss: 2.3731[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 40.938 	Loss: 1.9214[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 52.812 	Loss: 1.5478[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 63.312 	Loss: 1.1959[00m
[92m  Client6 Test => 	Acc: 40.690 	Loss: 2.2598[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 19.766 	Loss: 2.3707[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 27.812 	Loss: 2.1412[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 36.094 	Loss: 2.0249[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 39.922 	Loss: 1.8238[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 42.969 	Loss: 1.6645[00m
[92m  Client7 Test => 	Acc: 24.521 	Loss: 3.1514[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 18.099 	Loss: 2.5523[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 24.740 	Loss: 2.1375[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 39.844 	Loss: 1.9625[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 41.927 	Loss: 1.8573[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 29.948 	Loss: 2.3771[00m
[92m  Client8 Test => 	Acc: 4.292 	Loss: 4.1145[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 24.099 	Loss: 2.1964[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 45.793 	Loss: 1.6554[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 50.901 	Loss: 1.9299[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 50.361 	Loss: 1.5711[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 64.303 	Loss: 1.1818[00m
[92m  Client9 Test => 	Acc: 47.322 	Loss: 2.3961[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Client-side aggregation done
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Server-side aggregation done
[92m  Client0 Test => 	Acc: 2.194 	Loss: 5.3909[00m
 Train: Round   1, Avg Accuracy 65.716 | Avg Loss 1.095
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.0049[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.208 	Loss: 4301.1386[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.0166[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client1 Test => 	Acc: 2.194 	Loss: 404.2399[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 34.201 	Loss: 2.4738[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 42.361 	Loss: 2.0957[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 44.184 	Loss: 1.8703[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 52.691 	Loss: 1.5626[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 57.118 	Loss: 1.3632[00m
[92m  Client2 Test => 	Acc: 30.938 	Loss: 3.3470[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 36.155 	Loss: 1.9890[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 64.106 	Loss: 1.2743[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 72.743 	Loss: 0.9990[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 73.134 	Loss: 0.9487[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 63.194 	Loss: 1.3618[00m
[92m  Client3 Test => 	Acc: 25.644 	Loss: 2.8697[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 17.773 	Loss: 2.9233[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 44.629 	Loss: 1.6546[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 43.945 	Loss: 1.6899[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 61.328 	Loss: 1.2084[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 75.000 	Loss: 0.8085[00m
[92m  Client4 Test => 	Acc: 40.972 	Loss: 2.8096[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 16.146 	Loss: 2.9613[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 27.734 	Loss: 2.1073[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 26.432 	Loss: 2.0187[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 41.276 	Loss: 1.8406[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 38.672 	Loss: 2.0189[00m
[92m  Client5 Test => 	Acc: 29.102 	Loss: 2.9010[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 26.438 	Loss: 2.3909[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 54.312 	Loss: 1.5293[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 63.781 	Loss: 1.1640[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 70.812 	Loss: 0.9303[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 73.938 	Loss: 0.8354[00m
[92m  Client6 Test => 	Acc: 55.920 	Loss: 1.6909[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 23.672 	Loss: 2.7932[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 21.797 	Loss: 2.2380[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 29.766 	Loss: 2.1081[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 35.000 	Loss: 1.9463[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 43.359 	Loss: 1.7028[00m
[92m  Client7 Test => 	Acc: 24.143 	Loss: 3.1709[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 13.411 	Loss: 3.4927[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 25.781 	Loss: 2.2893[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 23.307 	Loss: 2.1356[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 29.167 	Loss: 2.0918[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 47.005 	Loss: 1.7754[00m
[92m  Client8 Test => 	Acc: 14.848 	Loss: 3.4915[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 22.356 	Loss: 2.5455[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 36.719 	Loss: 1.8596[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 47.476 	Loss: 1.6317[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 63.221 	Loss: 1.2665[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 75.661 	Loss: 0.9024[00m
[92m  Client9 Test => 	Acc: 51.885 	Loss: 1.9225[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Client-side aggregation done
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Server-side aggregation done
[92m  Client0 Test => 	Acc: 13.832 	Loss: 2.6641[00m
 Train: Round   2, Avg Accuracy 67.395 | Avg Loss 1.077
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 94.585 	Loss: 0.2039[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00mC:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(

[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.194 	Loss: 2614163.1923[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 77.778 	Loss: 0.8845[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client1 Test => 	Acc: 2.208 	Loss: 36426.4502[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 40.538 	Loss: 2.4485[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 47.049 	Loss: 1.6592[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 59.462 	Loss: 1.3240[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 56.771 	Loss: 1.6288[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 60.503 	Loss: 1.3253[00m
[92m  Client2 Test => 	Acc: 37.999 	Loss: 3.2802[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 55.165 	Loss: 1.5193[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 71.788 	Loss: 0.9377[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 77.474 	Loss: 0.7763[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 81.076 	Loss: 0.5958[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 82.943 	Loss: 0.5517[00m
[92m  Client3 Test => 	Acc: 48.337 	Loss: 2.1576[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 30.371 	Loss: 2.1577[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 48.340 	Loss: 1.5403[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 65.527 	Loss: 1.0304[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 70.801 	Loss: 0.8704[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 79.883 	Loss: 0.6389[00m
[92m  Client4 Test => 	Acc: 34.510 	Loss: 5.2933[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 24.219 	Loss: 2.3841[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 50.521 	Loss: 1.7239[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 61.589 	Loss: 1.2576[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 67.057 	Loss: 1.0602[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 71.745 	Loss: 0.9192[00m
[92m  Client5 Test => 	Acc: 41.364 	Loss: 2.7075[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 36.000 	Loss: 2.1418[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 50.344 	Loss: 1.7045[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 68.688 	Loss: 1.0494[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 77.125 	Loss: 0.7187[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 81.719 	Loss: 0.6191[00m
[92m  Client6 Test => 	Acc: 57.970 	Loss: 1.5624[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 30.625 	Loss: 2.3291[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 42.344 	Loss: 1.7083[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 48.984 	Loss: 1.5119[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 57.344 	Loss: 1.2672[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 60.391 	Loss: 1.1647[00m
[92m  Client7 Test => 	Acc: 36.263 	Loss: 2.7352[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 18.620 	Loss: 2.5845[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 39.844 	Loss: 2.0869[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 50.911 	Loss: 1.5851[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 35.547 	Loss: 1.9842[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 37.891 	Loss: 1.8229[00m
[92m  Client8 Test => 	Acc: 17.245 	Loss: 3.0735[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 35.096 	Loss: 2.1336[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 53.486 	Loss: 1.5077[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 67.668 	Loss: 1.0626[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 76.142 	Loss: 0.8085[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 78.786 	Loss: 0.7034[00m
[92m  Client9 Test => 	Acc: 53.970 	Loss: 1.9519[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Client-side aggregation done
Traceback (most recent call last):
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\main.py", line 10, in <module>
    process.main(args)
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\process.py", line 53, in main
    loss_train, acc_train, loss_test, acc_test, client_train_carbon, server_train_carbon, client_agg_carbon, server_agg_carbon, uplink_data, downlink_data, round_class = Split_Fed(args, trainData, testData)
                                                                                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\algorithms.py", line 236, in Split_Fed
    w_glob_server, t = mudhog_server.aggregator(w_locals_server, server)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\mudhog.py", line 44, in aggregator
    weight_vals = self.calculator(*self.get_hogs(server, 'server'))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\mudhog.py", line 196, in calculator
    value_sHoGs = pca.fit_transform(value_sHoGs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\utils\_set_output.py", line 313, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\decomposition\_pca.py", line 474, in fit_transform
    U, S, _, X, x_is_centered, xp = self._fit(X)
                                    ^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\decomposition\_pca.py", line 547, in _fit
    return self._fit_full(X, n_components, xp, is_array_api_compliant)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\decomposition\_pca.py", line 561, in _fit_full
    raise ValueError(
ValueError: n_components=10 must be between 0 and min(n_samples, n_features)=8 with svd_solver='full'
===== END Thu 01/15/2026 12:13:25.23 ===== 
