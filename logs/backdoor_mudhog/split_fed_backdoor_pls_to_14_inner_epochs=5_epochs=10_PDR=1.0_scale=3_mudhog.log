===== START Thu 01/15/2026 12:13:25.23 ===== 
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
[codecarbon INFO @ 12:13:58] [setup] RAM Tracking...
[codecarbon INFO @ 12:13:58] [setup] CPU Tracking...
[codecarbon WARNING @ 12:13:58] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 12:13:59] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 12:13:59] [setup] GPU Tracking...
[codecarbon INFO @ 12:13:59] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 12:13:59] >>> Tracker's metadata:
[codecarbon INFO @ 12:13:59]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 12:13:59]   Python version: 3.12.7
[codecarbon INFO @ 12:13:59]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 12:13:59]   Available RAM : 126.630 GB
[codecarbon INFO @ 12:13:59]   CPU count: 56
[codecarbon INFO @ 12:13:59]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 12:13:59]   GPU count: 2
[codecarbon INFO @ 12:13:59]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 12:14:02] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 12:14:04] Energy consumed for RAM : 0.000019 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 12:14:04] Energy consumed for all CPUs : 0.000042 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 12:14:04] Energy consumed for all GPUs : 0.000007 kWh. Total GPU Power : 17.106059799747186 W
[codecarbon INFO @ 12:14:04] 0.000068 kWh of electricity used since the beginning.
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
[codecarbon INFO @ 12:40:45] [setup] RAM Tracking...
[codecarbon INFO @ 12:40:45] [setup] CPU Tracking...
[codecarbon WARNING @ 12:40:45] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 12:40:47] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 12:40:47] [setup] GPU Tracking...
[codecarbon INFO @ 12:40:47] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 12:40:47] >>> Tracker's metadata:
[codecarbon INFO @ 12:40:47]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 12:40:47]   Python version: 3.12.7
[codecarbon INFO @ 12:40:47]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 12:40:47]   Available RAM : 126.630 GB
[codecarbon INFO @ 12:40:47]   CPU count: 56
[codecarbon INFO @ 12:40:47]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 12:40:47]   GPU count: 2
[codecarbon INFO @ 12:40:47]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 12:40:50] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 12:40:51] Energy consumed for RAM : 0.000017 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 12:40:51] Energy consumed for all CPUs : 0.000037 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 12:40:51] Energy consumed for all GPUs : 0.000006 kWh. Total GPU Power : 16.394584618726395 W
[codecarbon INFO @ 12:40:51] 0.000061 kWh of electricity used since the beginning.
[codecarbon INFO @ 12:41:47] [setup] RAM Tracking...
[codecarbon INFO @ 12:41:47] [setup] CPU Tracking...
[codecarbon WARNING @ 12:41:47] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 12:41:48] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 12:41:48] [setup] GPU Tracking...
[codecarbon INFO @ 12:41:48] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 12:41:48] >>> Tracker's metadata:
[codecarbon INFO @ 12:41:48]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 12:41:48]   Python version: 3.12.7
[codecarbon INFO @ 12:41:48]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 12:41:48]   Available RAM : 126.630 GB
[codecarbon INFO @ 12:41:48]   CPU count: 56
[codecarbon INFO @ 12:41:48]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 12:41:48]   GPU count: 2
[codecarbon INFO @ 12:41:48]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 12:41:51] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 12:41:52] Energy consumed for RAM : 0.000011 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 12:41:52] Energy consumed for all CPUs : 0.000023 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 12:41:52] Energy consumed for all GPUs : 0.000004 kWh. Total GPU Power : 18.080006494677185 W
[codecarbon INFO @ 12:41:52] 0.000038 kWh of electricity used since the beginning.
################################################################
#                              batch_size: 64                  #
#                         test_batch_size: 64                  #
#                                  epochs: 10                  #
#                               optimizer: SGD                 #
#                                      lr: 0.001               #
#                                momentum: 0.5                 #
#                                    seed: 1                   #
#                             num_clients: 10                  #
#                                   scale: 3                   #
#                                 dataset: plant               #
#                             loader_type: dirichlet           #
#                                      AR: mudhog              #
#                                    side: both                #
#                                     PDR: 1.0                 #
#                                  attack: backdoor pls->14    #
#                          label_flipping: uni                 #
#                         experiment_name: split_fed_backdoor_pls_to_14_inner_epochs=5_epochs=10_PDR=1.0_scale=3_mudhog#
#                            inner_epochs: 5                   #
#                                   setup: split_fed           #
#                                   alpha: 0.5                 #
################################################################
NVIDIA RTX A5000
---------split_fed_backdoor_pls_to_14_inner_epochs=5_epochs=10_PDR=1.0_scale=3_mudhog----------
initialize a data loader
Using cuda
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 96.794 	Loss: 0.0954[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.194 	Loss: 83176492866.9538[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 89.844 	Loss: 0.2995[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client1 Test => 	Acc: 2.194 	Loss: 58695842201.6000[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 89.497 	Loss: 0.3144[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client2 Test => 	Acc: 2.194 	Loss: 50340226347.3231[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 25.694 	Loss: 8.5804[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 33.116 	Loss: 1.9272[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 63.759 	Loss: 1.2621[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 72.613 	Loss: 1.0028[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 73.003 	Loss: 0.9884[00m
[92m  Client3 Test => 	Acc: 29.891 	Loss: 3.4834[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 15.332 	Loss: 7.5291[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 21.191 	Loss: 2.5744[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 36.621 	Loss: 2.3239[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 50.195 	Loss: 1.4548[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 61.133 	Loss: 1.2203[00m
[92m  Client4 Test => 	Acc: 26.069 	Loss: 2.8970[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 17.318 	Loss: 7.6099[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 21.094 	Loss: 2.4238[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 19.531 	Loss: 2.1970[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 30.990 	Loss: 2.0187[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 35.156 	Loss: 1.9294[00m
[92m  Client5 Test => 	Acc: 9.570 	Loss: 3.7773[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 24.969 	Loss: 2.4801[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 48.031 	Loss: 1.7512[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 61.188 	Loss: 1.2717[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 70.000 	Loss: 0.9671[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 74.094 	Loss: 0.8228[00m
[92m  Client6 Test => 	Acc: 51.764 	Loss: 1.6332[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 20.312 	Loss: 2.8092[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 22.500 	Loss: 2.3874[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 24.141 	Loss: 2.1642[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 25.469 	Loss: 2.1176[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 33.828 	Loss: 1.9609[00m
[92m  Client7 Test => 	Acc: 24.016 	Loss: 3.3079[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 20.052 	Loss: 10.1290[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 23.438 	Loss: 2.2887[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 26.823 	Loss: 2.1203[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 27.604 	Loss: 2.1012[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 29.688 	Loss: 2.1701[00m
[92m  Client8 Test => 	Acc: 4.189 	Loss: 3.4216[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 20.493 	Loss: 2.8848[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 27.464 	Loss: 2.1292[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 30.108 	Loss: 2.0594[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 47.536 	Loss: 1.6471[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 48.618 	Loss: 1.5405[00m
[92m  Client9 Test => 	Acc: 24.007 	Loss: 2.7443[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[0. 0. 0. 1. 0. 1. 1. 1. 1. 1.]
Client-side aggregation done
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Server-side aggregation done
[92m  Client0 Test => 	Acc: 2.188 	Loss: 2.7656[00m
 Train: Round   0, Avg Accuracy 65.552 | Avg Loss 1.063
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.0954[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.194 	Loss: 928516.7029[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.3396[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client1 Test => 	Acc: 2.188 	Loss: 105806.4060[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.3363[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client2 Test => 	Acc: 2.201 	Loss: 141771.3204[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 37.717 	Loss: 1.9317[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 55.252 	Loss: 1.7305[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 72.222 	Loss: 1.0175[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 77.344 	Loss: 0.7942[00mC:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
[codecarbon INFO @ 13:08:39] [setup] RAM Tracking...
[codecarbon INFO @ 13:08:39] [setup] CPU Tracking...
[codecarbon WARNING @ 13:08:39] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 13:08:41] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 13:08:41] [setup] GPU Tracking...
[codecarbon INFO @ 13:08:41] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 13:08:41] >>> Tracker's metadata:
[codecarbon INFO @ 13:08:41]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 13:08:41]   Python version: 3.12.7
[codecarbon INFO @ 13:08:41]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 13:08:41]   Available RAM : 126.630 GB
[codecarbon INFO @ 13:08:41]   CPU count: 56
[codecarbon INFO @ 13:08:41]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 13:08:41]   GPU count: 2
[codecarbon INFO @ 13:08:41]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 13:08:44] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 13:08:45] Energy consumed for RAM : 0.000010 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 13:08:45] Energy consumed for all CPUs : 0.000022 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 13:08:45] Energy consumed for all GPUs : 0.000004 kWh. Total GPU Power : 17.12566750726078 W
[codecarbon INFO @ 13:08:45] 0.000036 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:09:40] [setup] RAM Tracking...
[codecarbon INFO @ 13:09:40] [setup] CPU Tracking...
[codecarbon WARNING @ 13:09:40] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 13:09:42] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 13:09:42] [setup] GPU Tracking...
[codecarbon INFO @ 13:09:42] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 13:09:42] >>> Tracker's metadata:
[codecarbon INFO @ 13:09:42]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 13:09:42]   Python version: 3.12.7
[codecarbon INFO @ 13:09:42]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 13:09:42]   Available RAM : 126.630 GB
[codecarbon INFO @ 13:09:42]   CPU count: 56
[codecarbon INFO @ 13:09:42]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 13:09:42]   GPU count: 2
[codecarbon INFO @ 13:09:42]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 13:09:45] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 13:09:45] Energy consumed for RAM : 0.000009 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 13:09:45] Energy consumed for all CPUs : 0.000020 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 13:09:45] Energy consumed for all GPUs : 0.000004 kWh. Total GPU Power : 18.65063878819663 W
[codecarbon INFO @ 13:09:45] 0.000033 kWh of electricity used since the beginning.
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
[codecarbon INFO @ 13:36:33] [setup] RAM Tracking...
[codecarbon INFO @ 13:36:33] [setup] CPU Tracking...
[codecarbon WARNING @ 13:36:33] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 13:36:35] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 13:36:35] [setup] GPU Tracking...
[codecarbon INFO @ 13:36:35] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 13:36:35] >>> Tracker's metadata:
[codecarbon INFO @ 13:36:35]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 13:36:35]   Python version: 3.12.7
[codecarbon INFO @ 13:36:35]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 13:36:35]   Available RAM : 126.630 GB
[codecarbon INFO @ 13:36:35]   CPU count: 56
[codecarbon INFO @ 13:36:35]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 13:36:35]   GPU count: 2
[codecarbon INFO @ 13:36:35]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 13:36:38] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 13:36:38] Energy consumed for RAM : 0.000010 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 13:36:38] Energy consumed for all CPUs : 0.000022 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 13:36:38] Energy consumed for all GPUs : 0.000003 kWh. Total GPU Power : 16.317175274126782 W
[codecarbon INFO @ 13:36:38] 0.000035 kWh of electricity used since the beginning.
[codecarbon INFO @ 13:37:34] [setup] RAM Tracking...
[codecarbon INFO @ 13:37:34] [setup] CPU Tracking...
[codecarbon WARNING @ 13:37:34] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 13:37:36] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 13:37:36] [setup] GPU Tracking...
[codecarbon INFO @ 13:37:36] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 13:37:36] >>> Tracker's metadata:
[codecarbon INFO @ 13:37:36]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 13:37:36]   Python version: 3.12.7
[codecarbon INFO @ 13:37:36]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 13:37:36]   Available RAM : 126.630 GB
[codecarbon INFO @ 13:37:36]   CPU count: 56
[codecarbon INFO @ 13:37:36]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 13:37:36]   GPU count: 2
[codecarbon INFO @ 13:37:36]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 13:37:39] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 13:37:40] Energy consumed for RAM : 0.000009 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 13:37:40] Energy consumed for all CPUs : 0.000019 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 13:37:40] Energy consumed for all GPUs : 0.000003 kWh. Total GPU Power : 16.562362240227202 W
[codecarbon INFO @ 13:37:40] 0.000031 kWh of electricity used since the beginning.

[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 80.165 	Loss: 0.6585[00m
[92m  Client3 Test => 	Acc: 41.896 	Loss: 2.1338[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 14.355 	Loss: 2.4989[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 34.082 	Loss: 1.8681[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 52.246 	Loss: 1.4718[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 63.965 	Loss: 1.0494[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 69.922 	Loss: 0.8892[00m
[92m  Client4 Test => 	Acc: 38.398 	Loss: 3.1169[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 14.844 	Loss: 2.4222[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 32.031 	Loss: 2.0377[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 37.630 	Loss: 1.8260[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 52.474 	Loss: 1.5456[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 62.500 	Loss: 1.2531[00m
[92m  Client5 Test => 	Acc: 32.851 	Loss: 3.1804[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 26.125 	Loss: 2.1522[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 45.719 	Loss: 1.8026[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 66.469 	Loss: 1.1533[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 68.750 	Loss: 1.0027[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 70.781 	Loss: 0.9686[00m
[92m  Client6 Test => 	Acc: 44.108 	Loss: 1.9221[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 17.109 	Loss: 2.4163[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 29.062 	Loss: 2.0972[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 36.641 	Loss: 1.9645[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 47.812 	Loss: 1.5891[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 41.172 	Loss: 1.8746[00m
[92m  Client7 Test => 	Acc: 28.518 	Loss: 3.0254[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 11.328 	Loss: 2.6230[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 28.255 	Loss: 2.1336[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 34.505 	Loss: 2.1836[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 27.995 	Loss: 2.2338[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 29.818 	Loss: 2.1101[00m
[92m  Client8 Test => 	Acc: 4.856 	Loss: 3.4434[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 19.471 	Loss: 2.3328[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 36.599 	Loss: 1.9714[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 53.245 	Loss: 1.4884[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 44.712 	Loss: 2.0520[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 57.512 	Loss: 1.4502[00m
[92m  Client9 Test => 	Acc: 45.763 	Loss: 2.4520[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Client-side aggregation done
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Server-side aggregation done
[92m  Client0 Test => 	Acc: 2.188 	Loss: 19.3042[00m
 Train: Round   1, Avg Accuracy 71.187 | Avg Loss 0.920
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.201 	Loss: 29.5652[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client1 Test => 	Acc: 2.188 	Loss: 28.5684[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client2 Test => 	Acc: 2.201 	Loss: 29.1139[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 50.608 	Loss: 2.4206[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 72.917 	Loss: 0.9519[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 74.609 	Loss: 0.8868[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 80.252 	Loss: 0.6723[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 81.858 	Loss: 0.5742[00m
[92m  Client3 Test => 	Acc: 42.683 	Loss: 1.9710[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 25.586 	Loss: 4.2551[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 53.320 	Loss: 1.4364[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 58.203 	Loss: 1.2283[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 64.160 	Loss: 1.0875[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 75.293 	Loss: 0.7827[00m
[92m  Client4 Test => 	Acc: 42.075 	Loss: 3.3688[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 18.099 	Loss: 5.2972[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 34.115 	Loss: 2.1008[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 49.349 	Loss: 1.7277[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 55.729 	Loss: 1.4155[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 66.536 	Loss: 1.0466[00m
[92m  Client5 Test => 	Acc: 41.560 	Loss: 3.0078[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 21.750 	Loss: 3.3408[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 24.688 	Loss: 2.1178[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 45.375 	Loss: 1.8167[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 55.219 	Loss: 1.5257[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 44.938 	Loss: 1.8644[00m
[92m  Client6 Test => 	Acc: 28.559 	Loss: 3.5166[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 25.469 	Loss: 3.8624[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 32.422 	Loss: 2.0594[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 37.578 	Loss: 1.7596[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 50.234 	Loss: 1.4492[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 54.219 	Loss: 1.4278[00m
[92m  Client7 Test => 	Acc: 35.452 	Loss: 2.7574[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 10.547 	Loss: 6.0690[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 33.984 	Loss: 2.2630[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 33.724 	Loss: 2.1093[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 41.016 	Loss: 1.8006[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 58.203 	Loss: 1.4607[00m
[92m  Client8 Test => 	Acc: 24.542 	Loss: 3.0868[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 26.262 	Loss: 3.6149[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 44.772 	Loss: 1.7436[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 64.303 	Loss: 1.2307[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 73.858 	Loss: 0.8926[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 79.567 	Loss: 0.6998[00m
[92m  Client9 Test => 	Acc: 55.360 	Loss: 1.6929[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Client-side aggregation done
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Server-side aggregation done
[92m  Client0 Test => 	Acc: 21.081 	Loss: 2.5616[00m
 Train: Round   2, Avg Accuracy 76.061 | Avg Loss 0.786
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 93.103 	Loss: 0.1984[00mC:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(

[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.188 	Loss: 92058071859.2000[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 73.872 	Loss: 0.8757[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client1 Test => 	Acc: 2.194 	Loss: 830728880.2462[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 72.135 	Loss: 0.9183[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client2 Test => 	Acc: 2.188 	Loss: 2668596271.2615[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 62.847 	Loss: 1.3896[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 77.995 	Loss: 0.7311[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 81.293 	Loss: 0.6015[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 83.160 	Loss: 0.5427[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 85.807 	Loss: 0.4481[00m
[92m  Client3 Test => 	Acc: 56.528 	Loss: 1.9121[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 38.672 	Loss: 1.9442[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 63.184 	Loss: 1.1323[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 73.926 	Loss: 0.8049[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 79.395 	Loss: 0.6362[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 81.738 	Loss: 0.5992[00m
[92m  Client4 Test => 	Acc: 46.006 	Loss: 2.2892[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 30.469 	Loss: 2.3283[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 58.594 	Loss: 1.3314[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 66.146 	Loss: 1.0612[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 67.318 	Loss: 1.0351[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 73.698 	Loss: 0.8151[00m
[92m  Client5 Test => 	Acc: 45.141 	Loss: 2.5365[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 48.344 	Loss: 1.7741[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 68.969 	Loss: 0.9888[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 76.281 	Loss: 0.7361[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 82.219 	Loss: 0.5853[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 86.969 	Loss: 0.4491[00m
[92m  Client6 Test => 	Acc: 61.484 	Loss: 1.3181[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 36.719 	Loss: 2.2079[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 49.688 	Loss: 1.4656[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 62.500 	Loss: 1.1455[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 65.391 	Loss: 1.0159[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 73.516 	Loss: 0.8328[00m
[92m  Client7 Test => 	Acc: 43.421 	Loss: 2.1919[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 27.474 	Loss: 2.4286[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 58.854 	Loss: 1.3951[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 75.781 	Loss: 0.8775[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 76.693 	Loss: 0.7382[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 80.469 	Loss: 0.6602[00m
[92m  Client8 Test => 	Acc: 43.372 	Loss: 2.2817[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 45.252 	Loss: 1.9700[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 69.351 	Loss: 1.0341[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 76.803 	Loss: 0.7907[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 83.053 	Loss: 0.5788[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 79.147 	Loss: 0.7255[00m
[92m  Client9 Test => 	Acc: 56.074 	Loss: 1.4803[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Client-side aggregation done
Traceback (most recent call last):
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\main.py", line 10, in <module>
    process.main(args)
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\process.py", line 53, in main
    loss_train, acc_train, loss_test, acc_test, client_train_carbon, server_train_carbon, client_agg_carbon, server_agg_carbon, uplink_data, downlink_data, round_class = Split_Fed(args, trainData, testData)
                                                                                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\algorithms.py", line 236, in Split_Fed
    w_glob_server, t = mudhog_server.aggregator(w_locals_server, server)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\mudhog.py", line 44, in aggregator
    weight_vals = self.calculator(*self.get_hogs(server, 'server'))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\mudhog.py", line 196, in calculator
    value_sHoGs = pca.fit_transform(value_sHoGs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\utils\_set_output.py", line 313, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\decomposition\_pca.py", line 474, in fit_transform
    U, S, _, X, x_is_centered, xp = self._fit(X)
                                    ^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\decomposition\_pca.py", line 547, in _fit
    return self._fit_full(X, n_components, xp, is_array_api_compliant)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\decomposition\_pca.py", line 561, in _fit_full
    raise ValueError(
ValueError: n_components=10 must be between 0 and min(n_samples, n_features)=7 with svd_solver='full'
===== END Thu 01/15/2026 14:03:17.94 ===== 
