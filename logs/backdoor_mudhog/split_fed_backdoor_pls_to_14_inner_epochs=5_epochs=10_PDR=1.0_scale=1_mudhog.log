===== START Thu 01/15/2026  8:33:37.71 ===== 
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
[codecarbon INFO @ 08:34:11] [setup] RAM Tracking...
[codecarbon INFO @ 08:34:11] [setup] CPU Tracking...
[codecarbon WARNING @ 08:34:11] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 08:34:12] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 08:34:12] [setup] GPU Tracking...
[codecarbon INFO @ 08:34:12] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 08:34:12] >>> Tracker's metadata:
[codecarbon INFO @ 08:34:12]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 08:34:12]   Python version: 3.12.7
[codecarbon INFO @ 08:34:12]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 08:34:12]   Available RAM : 126.630 GB
[codecarbon INFO @ 08:34:12]   CPU count: 56
[codecarbon INFO @ 08:34:12]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 08:34:12]   GPU count: 2
[codecarbon INFO @ 08:34:12]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 08:34:16] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 08:34:17] Energy consumed for RAM : 0.000019 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 08:34:17] Energy consumed for all CPUs : 0.000041 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 08:34:17] Energy consumed for all GPUs : 0.000008 kWh. Total GPU Power : 19.531333127553573 W
[codecarbon INFO @ 08:34:17] 0.000068 kWh of electricity used since the beginning.
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
[codecarbon INFO @ 09:01:02] [setup] RAM Tracking...
[codecarbon INFO @ 09:01:02] [setup] CPU Tracking...
[codecarbon WARNING @ 09:01:02] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 09:01:03] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:01:03] [setup] GPU Tracking...
[codecarbon INFO @ 09:01:03] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 09:01:03] >>> Tracker's metadata:
[codecarbon INFO @ 09:01:03]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 09:01:03]   Python version: 3.12.7
[codecarbon INFO @ 09:01:03]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 09:01:03]   Available RAM : 126.630 GB
[codecarbon INFO @ 09:01:03]   CPU count: 56
[codecarbon INFO @ 09:01:03]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:01:03]   GPU count: 2
[codecarbon INFO @ 09:01:03]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 09:01:07] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 09:01:08] Energy consumed for RAM : 0.000016 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 09:01:08] Energy consumed for all CPUs : 0.000034 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 09:01:08] Energy consumed for all GPUs : 0.000005 kWh. Total GPU Power : 16.466856315374475 W
[codecarbon INFO @ 09:01:08] 0.000055 kWh of electricity used since the beginning.
[codecarbon INFO @ 09:02:03] [setup] RAM Tracking...
[codecarbon INFO @ 09:02:03] [setup] CPU Tracking...
[codecarbon WARNING @ 09:02:03] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 09:02:04] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:02:04] [setup] GPU Tracking...
[codecarbon INFO @ 09:02:04] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 09:02:04] >>> Tracker's metadata:
[codecarbon INFO @ 09:02:04]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 09:02:04]   Python version: 3.12.7
[codecarbon INFO @ 09:02:04]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 09:02:04]   Available RAM : 126.630 GB
[codecarbon INFO @ 09:02:04]   CPU count: 56
[codecarbon INFO @ 09:02:04]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:02:04]   GPU count: 2
[codecarbon INFO @ 09:02:04]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 09:02:07] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 09:02:08] Energy consumed for RAM : 0.000011 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 09:02:08] Energy consumed for all CPUs : 0.000023 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 09:02:08] Energy consumed for all GPUs : 0.000004 kWh. Total GPU Power : 18.286728115727104 W
[codecarbon INFO @ 09:02:08] 0.000038 kWh of electricity used since the beginning.
################################################################
#                              batch_size: 64                  #
#                         test_batch_size: 64                  #
#                                  epochs: 10                  #
#                               optimizer: SGD                 #
#                                      lr: 0.001               #
#                                momentum: 0.5                 #
#                                    seed: 1                   #
#                             num_clients: 10                  #
#                                   scale: 1                   #
#                                 dataset: plant               #
#                             loader_type: dirichlet           #
#                                      AR: mudhog              #
#                                    side: both                #
#                                     PDR: 1.0                 #
#                                  attack: backdoor pls->14    #
#                          label_flipping: uni                 #
#                         experiment_name: split_fed_backdoor_pls_to_14_inner_epochs=5_epochs=10_PDR=1.0_scale=1_mudhog#
#                            inner_epochs: 5                   #
#                                   setup: split_fed           #
#                                   alpha: 0.5                 #
################################################################
NVIDIA RTX A5000
---------split_fed_backdoor_pls_to_14_inner_epochs=5_epochs=10_PDR=1.0_scale=1_mudhog----------
initialize a data loader
Using cuda
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 96.794 	Loss: 0.0954[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.194 	Loss: 83176492866.9538[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 22.222 	Loss: 2.9358[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 22.483 	Loss: 2.1644[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 29.861 	Loss: 2.1398[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 29.948 	Loss: 2.1184[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 29.601 	Loss: 2.1269[00m
[92m  Client1 Test => 	Acc: 11.040 	Loss: 3.7074[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 22.049 	Loss: 6.6551[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 32.205 	Loss: 2.3310[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 42.622 	Loss: 1.9565[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 42.969 	Loss: 1.7749[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 46.354 	Loss: 1.7176[00m
[92m  Client2 Test => 	Acc: 15.758 	Loss: 4.0026[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 28.385 	Loss: 6.4374[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 40.278 	Loss: 1.8496[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 51.519 	Loss: 1.8758[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 66.797 	Loss: 1.1643[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 69.358 	Loss: 1.0766[00m
[92m  Client3 Test => 	Acc: 26.093 	Loss: 2.7934[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 17.773 	Loss: 3.3690[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 16.992 	Loss: 2.5190[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 35.352 	Loss: 2.0378[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 45.605 	Loss: 1.6341[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 48.145 	Loss: 1.6025[00m
[92m  Client4 Test => 	Acc: 21.540 	Loss: 3.2315[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 23.828 	Loss: 3.9590[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 32.552 	Loss: 2.5187[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 44.010 	Loss: 1.9113[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 39.714 	Loss: 2.3008[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 48.438 	Loss: 1.6433[00m
[92m  Client5 Test => 	Acc: 26.701 	Loss: 3.2420[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 27.219 	Loss: 3.7328[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 50.750 	Loss: 1.6571[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 61.562 	Loss: 1.2814[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 67.625 	Loss: 1.0270[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 71.938 	Loss: 0.8808[00m
[92m  Client6 Test => 	Acc: 53.880 	Loss: 1.7449[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 21.328 	Loss: 3.4902[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 23.984 	Loss: 2.3346[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 28.672 	Loss: 2.1237[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 38.438 	Loss: 1.8386[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 45.781 	Loss: 1.5794[00m
[92m  Client7 Test => 	Acc: 27.430 	Loss: 2.8600[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 21.615 	Loss: 2.7946[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 25.521 	Loss: 2.3765[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 22.135 	Loss: 2.1736[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 25.391 	Loss: 2.1811[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 30.339 	Loss: 1.9945[00m
[92m  Client8 Test => 	Acc: 8.739 	Loss: 3.5862[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 20.733 	Loss: 5.8081[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 23.918 	Loss: 2.3309[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 26.442 	Loss: 2.1123[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 37.861 	Loss: 1.8701[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 57.212 	Loss: 1.3873[00m
[92m  Client9 Test => 	Acc: 38.047 	Loss: 2.9313[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]
Client-side aggregation done
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Server-side aggregation done
[92m  Client0 Test => 	Acc: 12.956 	Loss: 2.7301[00m
 Train: Round   0, Avg Accuracy 54.716 | Avg Loss 1.401
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 93.103 	Loss: 0.1950[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.208 	Loss: 9917298.7692[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 19.097 	Loss: 2.3774[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 24.045 	Loss: 2.1966[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 35.503 	Loss: 1.9135[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 40.365 	Loss: 1.7514[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 47.135 	Loss: 1.5974[00m
[92m  Client1 Test => 	Acc: 25.214 	Loss: 3.2284[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 42.535 	Loss: 2.2477[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 43.316 	Loss: 1.8099[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 56.337 	Loss: 1.5089[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 60.590 	Loss: 1.2897[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 62.066 	Loss: 1.1699[00m
[92m  Client2 Test => 	Acc: 33.648 	Loss: 3.6728[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 32.639 	Loss: 2.0232[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 55.252 	Loss: 1.4671[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 69.618 	Loss: 1.0820[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 73.655 	Loss: 0.9632[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 76.649 	Loss: 0.8194[00mC:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
[codecarbon INFO @ 09:28:53] [setup] RAM Tracking...
[codecarbon INFO @ 09:28:53] [setup] CPU Tracking...
[codecarbon WARNING @ 09:28:53] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 09:28:55] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:28:55] [setup] GPU Tracking...
[codecarbon INFO @ 09:28:55] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 09:28:55] >>> Tracker's metadata:
[codecarbon INFO @ 09:28:55]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 09:28:55]   Python version: 3.12.7
[codecarbon INFO @ 09:28:55]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 09:28:55]   Available RAM : 126.630 GB
[codecarbon INFO @ 09:28:55]   CPU count: 56
[codecarbon INFO @ 09:28:55]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:28:55]   GPU count: 2
[codecarbon INFO @ 09:28:55]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 09:28:58] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 09:28:58] Energy consumed for RAM : 0.000010 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 09:28:58] Energy consumed for all CPUs : 0.000021 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 09:28:58] Energy consumed for all GPUs : 0.000003 kWh. Total GPU Power : 15.445943090480846 W
[codecarbon INFO @ 09:28:58] 0.000034 kWh of electricity used since the beginning.
[codecarbon INFO @ 09:29:53] [setup] RAM Tracking...
[codecarbon INFO @ 09:29:53] [setup] CPU Tracking...
[codecarbon WARNING @ 09:29:53] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 09:29:54] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:29:54] [setup] GPU Tracking...
[codecarbon INFO @ 09:29:54] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 09:29:54] >>> Tracker's metadata:
[codecarbon INFO @ 09:29:54]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 09:29:54]   Python version: 3.12.7
[codecarbon INFO @ 09:29:54]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 09:29:54]   Available RAM : 126.630 GB
[codecarbon INFO @ 09:29:54]   CPU count: 56
[codecarbon INFO @ 09:29:54]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:29:54]   GPU count: 2
[codecarbon INFO @ 09:29:54]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 09:29:58] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 09:29:58] Energy consumed for RAM : 0.000010 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 09:29:58] Energy consumed for all CPUs : 0.000022 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 09:29:58] Energy consumed for all GPUs : 0.000004 kWh. Total GPU Power : 17.16482905082117 W
[codecarbon INFO @ 09:29:58] 0.000035 kWh of electricity used since the beginning.
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(
[codecarbon INFO @ 09:56:39] [setup] RAM Tracking...
[codecarbon INFO @ 09:56:39] [setup] CPU Tracking...
[codecarbon WARNING @ 09:56:39] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 09:56:41] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:56:41] [setup] GPU Tracking...
[codecarbon INFO @ 09:56:41] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 09:56:41] >>> Tracker's metadata:
[codecarbon INFO @ 09:56:41]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 09:56:41]   Python version: 3.12.7
[codecarbon INFO @ 09:56:41]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 09:56:41]   Available RAM : 126.630 GB
[codecarbon INFO @ 09:56:41]   CPU count: 56
[codecarbon INFO @ 09:56:41]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:56:41]   GPU count: 2
[codecarbon INFO @ 09:56:41]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 09:56:44] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 09:56:44] Energy consumed for RAM : 0.000010 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 09:56:44] Energy consumed for all CPUs : 0.000021 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 09:56:44] Energy consumed for all GPUs : 0.000003 kWh. Total GPU Power : 15.836699960864626 W
[codecarbon INFO @ 09:56:44] 0.000034 kWh of electricity used since the beginning.
[codecarbon INFO @ 09:57:40] [setup] RAM Tracking...
[codecarbon INFO @ 09:57:40] [setup] CPU Tracking...
[codecarbon WARNING @ 09:57:40] No CPU tracking mode found. Falling back on CPU constant mode. 
 Windows OS detected: Please install Intel Power Gadget to measure CPU

[codecarbon INFO @ 09:57:42] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:57:42] [setup] GPU Tracking...
[codecarbon INFO @ 09:57:42] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 09:57:42] >>> Tracker's metadata:
[codecarbon INFO @ 09:57:42]   Platform system: Windows-11-10.0.22631-SP0
[codecarbon INFO @ 09:57:42]   Python version: 3.12.7
[codecarbon INFO @ 09:57:42]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 09:57:42]   Available RAM : 126.630 GB
[codecarbon INFO @ 09:57:42]   CPU count: 56
[codecarbon INFO @ 09:57:42]   CPU model: Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz
[codecarbon INFO @ 09:57:42]   GPU count: 2
[codecarbon INFO @ 09:57:42]   GPU model: 2 x NVIDIA RTX A5000
[codecarbon INFO @ 09:57:45] Saving emissions data to file C:\Users\pr8pf\Documents\GitHub\SplitFed\emissions.csv
[codecarbon INFO @ 09:57:46] Energy consumed for RAM : 0.000007 kWh. RAM Power : 47.48608875274658 W
[codecarbon INFO @ 09:57:46] Energy consumed for all CPUs : 0.000015 kWh. Total CPU Power : 102.50000000000001 W
[codecarbon INFO @ 09:57:46] Energy consumed for all GPUs : 0.000003 kWh. Total GPU Power : 17.168046717266655 W
[codecarbon INFO @ 09:57:46] 0.000025 kWh of electricity used since the beginning.

[92m  Client3 Test => 	Acc: 33.037 	Loss: 2.2195[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 18.750 	Loss: 2.4397[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 37.891 	Loss: 1.8742[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 52.734 	Loss: 1.4639[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 57.031 	Loss: 1.2632[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 62.500 	Loss: 1.1387[00m
[92m  Client4 Test => 	Acc: 34.798 	Loss: 2.8374[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 18.750 	Loss: 2.5267[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 32.812 	Loss: 1.9777[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 48.698 	Loss: 1.5733[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 48.047 	Loss: 1.5679[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 56.901 	Loss: 1.2928[00m
[92m  Client5 Test => 	Acc: 28.888 	Loss: 2.9394[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 23.844 	Loss: 2.2103[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 44.219 	Loss: 1.7810[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 63.375 	Loss: 1.2155[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 72.438 	Loss: 0.9135[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 75.094 	Loss: 0.8539[00m
[92m  Client6 Test => 	Acc: 50.594 	Loss: 1.6555[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 20.625 	Loss: 2.3961[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 27.422 	Loss: 2.1842[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 25.859 	Loss: 2.4285[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 28.516 	Loss: 2.1314[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 35.781 	Loss: 1.9278[00m
[92m  Client7 Test => 	Acc: 14.343 	Loss: 3.3305[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 18.359 	Loss: 2.4823[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 31.380 	Loss: 2.1108[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 42.057 	Loss: 2.0809[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 36.458 	Loss: 1.8023[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 53.125 	Loss: 1.5418[00m
[92m  Client8 Test => 	Acc: 17.104 	Loss: 2.6230[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 20.974 	Loss: 2.2893[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 27.344 	Loss: 2.0537[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 41.947 	Loss: 1.7315[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 54.267 	Loss: 1.3850[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 65.925 	Loss: 1.1699[00m
[92m  Client9 Test => 	Acc: 50.708 	Loss: 2.0118[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Client-side aggregation done
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Server-side aggregation done
[92m  Client0 Test => 	Acc: 2.188 	Loss: 2.8332[00m
 Train: Round   1, Avg Accuracy 63.518 | Avg Loss 1.151
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 100.000 	Loss: 0.0769[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.194 	Loss: 828939.5087[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 24.045 	Loss: 2.3771[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 39.844 	Loss: 1.7723[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 36.806 	Loss: 2.1102[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 45.052 	Loss: 1.7099[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 54.601 	Loss: 1.3707[00m
[92m  Client1 Test => 	Acc: 29.703 	Loss: 3.0588[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 38.976 	Loss: 2.2054[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 47.743 	Loss: 1.6062[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 54.601 	Loss: 1.3656[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 63.194 	Loss: 1.1379[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 65.451 	Loss: 1.0704[00m
[92m  Client2 Test => 	Acc: 39.482 	Loss: 2.4576[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 37.500 	Loss: 1.9395[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 56.467 	Loss: 1.4362[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 68.099 	Loss: 1.0984[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 77.821 	Loss: 0.7734[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 81.076 	Loss: 0.6260[00m
[92m  Client3 Test => 	Acc: 50.172 	Loss: 2.1853[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 18.164 	Loss: 2.4765[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 32.324 	Loss: 1.9521[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 51.660 	Loss: 1.4903[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 58.691 	Loss: 1.2264[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 63.477 	Loss: 1.0628[00m
[92m  Client4 Test => 	Acc: 34.929 	Loss: 3.1924[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 15.755 	Loss: 2.4770[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 29.036 	Loss: 2.0022[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 42.708 	Loss: 1.7618[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 59.505 	Loss: 1.2991[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 60.938 	Loss: 1.2371[00m
[92m  Client5 Test => 	Acc: 33.232 	Loss: 2.9341[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 22.938 	Loss: 2.2360[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 45.406 	Loss: 1.7151[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 49.062 	Loss: 1.7340[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 66.125 	Loss: 1.1245[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 71.156 	Loss: 0.9007[00m
[92m  Client6 Test => 	Acc: 54.244 	Loss: 1.7942[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 19.375 	Loss: 2.3027[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 30.469 	Loss: 2.1094[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 41.641 	Loss: 1.7763[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 45.156 	Loss: 1.6337[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 53.594 	Loss: 1.3991[00m
[92m  Client7 Test => 	Acc: 36.708 	Loss: 3.1836[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 14.062 	Loss: 2.6424[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 23.307 	Loss: 2.1636[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 28.906 	Loss: 2.1099[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 30.339 	Loss: 1.9941[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 54.557 	Loss: 1.5021[00m
[92m  Client8 Test => 	Acc: 25.662 	Loss: 3.4459[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 22.296 	Loss: 2.1983[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 46.334 	Loss: 1.6573[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 62.320 	Loss: 1.2398[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 70.733 	Loss: 1.0447[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 62.079 	Loss: 1.3227[00m
[92m  Client9 Test => 	Acc: 40.814 	Loss: 2.1823[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Client-side aggregation done
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Server-side aggregation done
[92m  Client0 Test => 	Acc: 12.709 	Loss: 2.7050[00m
 Train: Round   2, Avg Accuracy 66.693 | Avg Loss 1.049
[91m Client0 Train => Local Epoch: 0 / 5 	Acc: 96.552 	Loss: 0.0979[00m
[91m Client0 Train => Local Epoch: 1 / 5 	Acc: 100.000 	Loss: 0.0000[00mC:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\cluster\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.
  warnings.warn(

[91m Client0 Train => Local Epoch: 2 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 3 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[91m Client0 Train => Local Epoch: 4 / 5 	Acc: 100.000 	Loss: 0.0000[00m
[92m  Client0 Test => 	Acc: 2.194 	Loss: 44015022.9538[00m
[91m Client1 Train => Local Epoch: 0 / 5 	Acc: 29.861 	Loss: 2.2018[00m
[91m Client1 Train => Local Epoch: 1 / 5 	Acc: 45.573 	Loss: 1.8595[00m
[91m Client1 Train => Local Epoch: 2 / 5 	Acc: 25.347 	Loss: 2.4507[00m
[91m Client1 Train => Local Epoch: 3 / 5 	Acc: 29.688 	Loss: 2.1358[00m
[91m Client1 Train => Local Epoch: 4 / 5 	Acc: 27.865 	Loss: 2.1793[00m
[92m  Client1 Test => 	Acc: 11.074 	Loss: 3.3086[00m
[91m Client2 Train => Local Epoch: 0 / 5 	Acc: 43.750 	Loss: 1.9403[00m
[91m Client2 Train => Local Epoch: 1 / 5 	Acc: 50.608 	Loss: 1.5360[00m
[91m Client2 Train => Local Epoch: 2 / 5 	Acc: 57.292 	Loss: 1.3016[00m
[91m Client2 Train => Local Epoch: 3 / 5 	Acc: 61.719 	Loss: 1.1304[00m
[91m Client2 Train => Local Epoch: 4 / 5 	Acc: 69.358 	Loss: 0.9696[00m
[92m  Client2 Test => 	Acc: 46.982 	Loss: 2.3573[00m
[91m Client3 Train => Local Epoch: 0 / 5 	Acc: 58.594 	Loss: 1.4098[00m
[91m Client3 Train => Local Epoch: 1 / 5 	Acc: 75.564 	Loss: 0.8700[00m
[91m Client3 Train => Local Epoch: 2 / 5 	Acc: 81.293 	Loss: 0.6163[00m
[91m Client3 Train => Local Epoch: 3 / 5 	Acc: 80.903 	Loss: 0.6423[00m
[91m Client3 Train => Local Epoch: 4 / 5 	Acc: 83.333 	Loss: 0.5394[00m
[92m  Client3 Test => 	Acc: 54.817 	Loss: 2.2574[00m
[91m Client4 Train => Local Epoch: 0 / 5 	Acc: 27.246 	Loss: 2.0842[00m
[91m Client4 Train => Local Epoch: 1 / 5 	Acc: 46.777 	Loss: 1.5440[00m
[91m Client4 Train => Local Epoch: 2 / 5 	Acc: 61.719 	Loss: 1.2103[00m
[91m Client4 Train => Local Epoch: 3 / 5 	Acc: 65.625 	Loss: 1.0226[00m
[91m Client4 Train => Local Epoch: 4 / 5 	Acc: 70.020 	Loss: 0.9089[00m
[92m  Client4 Test => 	Acc: 38.568 	Loss: 2.7552[00m
[91m Client5 Train => Local Epoch: 0 / 5 	Acc: 32.682 	Loss: 2.2959[00m
[91m Client5 Train => Local Epoch: 1 / 5 	Acc: 46.615 	Loss: 1.7003[00m
[91m Client5 Train => Local Epoch: 2 / 5 	Acc: 60.286 	Loss: 1.2365[00m
[91m Client5 Train => Local Epoch: 3 / 5 	Acc: 50.260 	Loss: 1.7279[00m
[91m Client5 Train => Local Epoch: 4 / 5 	Acc: 54.818 	Loss: 1.2952[00m
[92m  Client5 Test => 	Acc: 36.711 	Loss: 2.4390[00m
[91m Client6 Train => Local Epoch: 0 / 5 	Acc: 38.406 	Loss: 2.0317[00m
[91m Client6 Train => Local Epoch: 1 / 5 	Acc: 24.750 	Loss: 2.1353[00m
[91m Client6 Train => Local Epoch: 2 / 5 	Acc: 26.781 	Loss: 2.0866[00m
[91m Client6 Train => Local Epoch: 3 / 5 	Acc: 25.000 	Loss: 2.0851[00m
[91m Client6 Train => Local Epoch: 4 / 5 	Acc: 25.875 	Loss: 2.0837[00m
[92m  Client6 Test => 	Acc: 11.074 	Loss: 3.1799[00m
[91m Client7 Train => Local Epoch: 0 / 5 	Acc: 28.203 	Loss: 2.1865[00m
[91m Client7 Train => Local Epoch: 1 / 5 	Acc: 41.562 	Loss: 1.7571[00m
[91m Client7 Train => Local Epoch: 2 / 5 	Acc: 42.109 	Loss: 1.6420[00m
[91m Client7 Train => Local Epoch: 3 / 5 	Acc: 44.922 	Loss: 1.5174[00m
[91m Client7 Train => Local Epoch: 4 / 5 	Acc: 49.297 	Loss: 1.3720[00m
[92m  Client7 Test => 	Acc: 33.353 	Loss: 2.7163[00m
[91m Client8 Train => Local Epoch: 0 / 5 	Acc: 28.516 	Loss: 2.5038[00m
[91m Client8 Train => Local Epoch: 1 / 5 	Acc: 44.922 	Loss: 1.7480[00m
[91m Client8 Train => Local Epoch: 2 / 5 	Acc: 59.375 	Loss: 1.3666[00m
[91m Client8 Train => Local Epoch: 3 / 5 	Acc: 66.927 	Loss: 1.0607[00m
[91m Client8 Train => Local Epoch: 4 / 5 	Acc: 70.964 	Loss: 0.8983[00m
[92m  Client8 Test => 	Acc: 39.265 	Loss: 2.3482[00m
[91m Client9 Train => Local Epoch: 0 / 5 	Acc: 32.031 	Loss: 2.0064[00m
[91m Client9 Train => Local Epoch: 1 / 5 	Acc: 50.120 	Loss: 1.5707[00m
[91m Client9 Train => Local Epoch: 2 / 5 	Acc: 67.368 	Loss: 1.0993[00m
[91m Client9 Train => Local Epoch: 3 / 5 	Acc: 75.841 	Loss: 0.8704[00m
[91m Client9 Train => Local Epoch: 4 / 5 	Acc: 81.791 	Loss: 0.6419[00m
[92m  Client9 Test => 	Acc: 57.255 	Loss: 1.5474[00m
------------------------------------------------
------ Federation process at Server-Side ------- 
------------------------------------------------
-----------------------------------------------------------
------ FedServer: Federation process at Client-Side ------- 
-----------------------------------------------------------
[1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]
Client-side aggregation done
Traceback (most recent call last):
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\main.py", line 10, in <module>
    process.main(args)
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\process.py", line 53, in main
    loss_train, acc_train, loss_test, acc_test, client_train_carbon, server_train_carbon, client_agg_carbon, server_agg_carbon, uplink_data, downlink_data, round_class = Split_Fed(args, trainData, testData)
                                                                                                                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\algorithms.py", line 236, in Split_Fed
    w_glob_server, t = mudhog_server.aggregator(w_locals_server, server)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\mudhog.py", line 44, in aggregator
    weight_vals = self.calculator(*self.get_hogs(server, 'server'))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\Documents\GitHub\SplitFed\mudhog.py", line 196, in calculator
    value_sHoGs = pca.fit_transform(value_sHoGs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\utils\_set_output.py", line 313, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\decomposition\_pca.py", line 474, in fit_transform
    U, S, _, X, x_is_centered, xp = self._fit(X)
                                    ^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\decomposition\_pca.py", line 547, in _fit
    return self._fit_full(X, n_components, xp, is_array_api_compliant)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\pr8pf\AppData\Local\anaconda3\Lib\site-packages\sklearn\decomposition\_pca.py", line 561, in _fit_full
    raise ValueError(
ValueError: n_components=10 must be between 0 and min(n_samples, n_features)=9 with svd_solver='full'
===== END Thu 01/15/2026 10:23:21.26 ===== 
